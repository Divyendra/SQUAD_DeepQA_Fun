{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQUAD Question Answering Dataset - Basic RNN\n",
    "SQUAD - Stanford Question Answering Dataset is a new reading comprehension dataset. It consists of questions posed by crowd workers on a set of wikipedia articles where the answer to every question is a segment o text, or span, from the corresponding reading passage. There are 1,00,000+ question answer pairs on 500+ articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal definition of the SQUAD Question answering dataset\n",
    "1. Given a three tuple (Q ,P , ($a_{s},a_{e}$)), where Q is the question, P is the context paragraph and $a_{s}$ and $a_{e}$ are the start and end indices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process\n",
    "1. Both the question and the context vectors are first encoded into a LSTM. \n",
    "2. Word embeddings are passed onto the encoder that generated an attention vector which the decoder decodes to produce the final output. \n",
    "3. Can use 100 dimensional glove embeddings. \n",
    "4. Simple Baseline Model -> Encoder - decoder architecture. \n",
    "    1. Pointer networks and Coattention techniques result in significant improvements. \n",
    "5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function for the SQUAD Model. \n",
    "\n",
    "1. Use tf.nn.embedding_lookup() to get the embeddings. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "1. DONE - Dataset importing from JSON format\n",
    "2. DONE - Extracting data.\n",
    "3. DONE - Downloading the Glove Vector model for the Word 2 vectors. Try with the 50 length vector first and then increase that. \n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "# import nltk\n",
    "# import sklearn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "Q_SIZE = 40  #Upper bound of max question size\n",
    "C_SIZE = 250 #Upper bound of max context size\n",
    "\n",
    "## RNN Model Parameters\n",
    "DATA_CAP = 1000\n",
    "batch_size = 64\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "W2VEC_LEN = 50\n",
    "GLOVE_path = \"./preprocessing/data/glove.6B/glove.6B.50d.txt\"\n",
    "reader = csv.reader(open(GLOVE_path), delimiter=' ', quoting=csv.QUOTE_NONE) \n",
    "W2VEC = {line[0]: np.array(list(map(float, line[1: ]))) for line in reader}\n",
    "del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f = open('glove.6B.50d.txt')\n",
    "# wordList = []\n",
    "# embeddings = []\n",
    "# model = {}\n",
    "\n",
    "# for line in f:\n",
    "#     splitLine = line.split()\n",
    "#     word = splitLine[0]\n",
    "#     wordList.append(word)\n",
    "#     embedding = [float(val) for val in splitLine[1:]]\n",
    "#     embeddings.append(embedding)\n",
    "#     model[word] = embedding\n",
    "# print (\" Done: \",len(model),\" loaded\")\n",
    "\n",
    "# ## Word List and the Word vectors of all the words.\n",
    "# wordsList   = np.asarray(wordList)\n",
    "# wordVectors = np.asarray(embeddings, dtype = np.float32 )\n",
    "\n",
    "# W2VEC = wordVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset for questions and answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All should be same: 81403 81403 81403 81403\n"
     ]
    }
   ],
   "source": [
    "# from itertools import zip\n",
    "\n",
    "path = \"./preprocessing/data/squad/\"\n",
    "q_p = \"train.question\"\n",
    "c_p = \"train.context\"\n",
    "s_p = \"train.span\"\n",
    "\n",
    "q_l = []; c_l = []; s_l = []\n",
    "itr = 0\n",
    "\n",
    "with open(path+q_p) as q_f, \\\n",
    "     open(path+c_p) as c_f, \\\n",
    "     open(path+s_p) as s_f:\n",
    "            \n",
    "    for q, c, s in zip( q_f, c_f, s_f):\n",
    "        q_l.append(q), c_l.append(c); s_l.append(s)\n",
    "        itr += 1\n",
    "        \n",
    "print (\"All should be same:\", itr, len(q_l), len(c_l), len(s_l))\n",
    "samples = list(zip(q_l, c_l,s_l))\n",
    "\n",
    "# combined_samples, test_samples = train_test_split(samples, test_size = 0.2, random_state = 2)\n",
    "# train_samples, val_samples = train_test_split( combined_samples, test_size = 0.2, random_state = 2)\n",
    "\n",
    "train_samples = samples[:int(0.6*len(samples))]\n",
    "val_samples = samples[int(0.61*len(samples)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_samples = train_samples[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context_lengths = []\n",
    "# question_lengths = []\n",
    "# answer_lengths = []\n",
    "\n",
    "# for i in range(len(c_l)):\n",
    "#     context_lengths.append(len(vectorize(c_l[i])))\n",
    "\n",
    "# for i in range(len(q_l)):\n",
    "#     question_lengths.append(len(vectorize(q_l[i])))\n",
    "    \n",
    "# for i in range(len(s_l)):\n",
    "#     answer_lengths.append(len(vectorize(s_l[i])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# plt.hist(context_lengths)\n",
    "# plt.title(\"Context Lengths distribution\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(question_lengths)\n",
    "# plt.title(\"Question Lengths distribution\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.hist(answer_lengths)\n",
    "# plt.title(\"Answer Lengths distribution\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyper - Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RNN Model Parameters\n",
    "embed_size = W2VEC_LEN\n",
    "## Maximum length for the question\n",
    "maxSeqLen_net1 = Q_SIZE\n",
    "\n",
    "## Maximum length for the context paragraph\n",
    "maxSeqLen_net2 = C_SIZE\n",
    "maxSeqLen_decoder = maxSeqLen_net2\n",
    "\n",
    "hidden_size = 32\n",
    "\n",
    "## For fully connected layer\n",
    "hidden_layer_size = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print (s_l[0])\n",
    "# idx = 0\n",
    "# values = s_l[idx].split()\n",
    "# values[0] = int(values[0])\n",
    "# values[1] = int(values[1])\n",
    "# span = values[:2]     \n",
    "# print (span)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(sent, fill, clean=False):\n",
    "    'Takes a sentence and returns corresponing list of GloVecs'\n",
    "    if clean:\n",
    "        sent = _dataCleaning(sent)\n",
    "    words = sent.split(\" \")\n",
    "    words = words[:fill]  #Capping the context. Beware!!\n",
    "    vecs = np.empty((1,W2VEC_LEN))\n",
    "    \n",
    "    for w in words:\n",
    "        vec = W2VEC.get(w.lower(), None)\n",
    "        if vec is None:\n",
    "            vec = np.random.rand(W2VEC_LEN)\n",
    "        vec = vec.reshape((1,W2VEC_LEN))\n",
    "        vecs = np.concatenate((vecs, vec), axis=0)\n",
    "    \n",
    "    PADDING = np.zeros((1, W2VEC_LEN))\n",
    "    for _ in np.arange(fill - len(words)):\n",
    "        vecs = np.concatenate((vecs, PADDING), axis=0)\n",
    "    return vecs[1:]\n",
    "    \n",
    "def _dataCleaning(string):\n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.replace(\"<br />\", \" \")\n",
    "    # The following replacements are suggested in the paper\n",
    "    # BidAF (Seo et al., 2016)\n",
    "    string = string.replace(\"''\", '\" ')\n",
    "    string = string.replace(\"``\", '\" ')\n",
    "\n",
    "    return re.sub(strip_special_chars, \"\", string)\n",
    "\n",
    "#Vectorise all Questions and Contexts\n",
    "def get_batch(cnt=64):\n",
    "    \"\"\"\n",
    "    Returns Batch of 'cnt' elements from the dataset as vectorized numpy arrays\n",
    "    @return: (questions, answers, labels) : All are vectorized numpy arrays \n",
    "    questions shape would be (cnt, Q_SIZE, W2VEC_LEN)\n",
    "    Each entry of labels is a one-hot repr of span given\n",
    "    \n",
    "    The numpy concatenate function copies at every call and hence is 10X slower for large batches.\n",
    "    Traditional Python lists append is a better fit here.\n",
    "    \"\"\"\n",
    "    N = (len(q_l) if DATA_CAP is None else DATA_CAP)\n",
    "    batch_ids = list(np.random.randint(0, N+1, cnt))\n",
    "    q_vecs = []; c_vecs = []; label_vecs = []\n",
    "    \n",
    "    for idx in batch_ids:\n",
    "        try:\n",
    "            s_l[idx]\n",
    "            values = s_l[idx].split()\n",
    "            values[0] = int(values[0])\n",
    "            values[1] = int(values[1])\n",
    "            span = values[:2]\n",
    "        except Exception as e:\n",
    "            print(\"Id:\", idx)\n",
    "            print(traceback.format_exc())\n",
    "            raise e\n",
    "            span = [1000, 1000]\n",
    "            \n",
    "        if span[1] >= C_SIZE:\n",
    "            replacement = np.random.randint(0, N, 1)[0]\n",
    "            batch_ids.append(replacement)\n",
    "            continue\n",
    "        q_vec = vectorize(q_l[idx], fill=Q_SIZE)\n",
    "        q_vecs.append(q_vec)\n",
    "        c_vec = vectorize(c_l[idx], fill=C_SIZE, clean=True)\n",
    "        c_vecs.append(c_vec)\n",
    "        label_vec = np.zeros(C_SIZE)\n",
    "        label_vec[span] = 1\n",
    "        label_vecs.append(label_vec)\n",
    "        \n",
    "    q_vecs = np.array(q_vecs); c_vecs = np.array(c_vecs); label_vecs = np.array(label_vecs)\n",
    "    return q_vecs, c_vecs, label_vecs\n",
    "    #print(q_vecs.shape, c_vecs.shape, label_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the generator for the tensorflow model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To get embeddings of all the words based on the index of the word.\n",
    "def getEmbeds(sess, sentence, maxSeqLen):\n",
    "    \n",
    "    sentence = sentence[:maxSeqLen]\n",
    "    sent_ids = []\n",
    "    sentence_word2vecs = []\n",
    "    \n",
    "    if( len(sentence)>=maxSeqLen ):\n",
    "        sentence = sentence[:maxSeqLen]\n",
    "        \n",
    "    else:    \n",
    "        for i in range( maxSeqLen - len(sentence)):\n",
    "            sentence.append('.')\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        if (not sentence[i] in wordList):\n",
    "            sent_ids.append(np.random.randint(35000))\n",
    "        else:\n",
    "            sent_ids.append(wordList.index(sentence[i]))\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        sentence_word2vecs.append(wordVectors[sent_ids[i]])\n",
    "    return sentence_word2vecs\n",
    "\n",
    "    \n",
    "def dataCleaning(string):\n",
    "    \n",
    "    strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub( strip_special_chars, \"\", string.lower())\n",
    "\n",
    "\n",
    "def generator2(samples, session, batch_size = 32):\n",
    "    \n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            \n",
    "            labels = []\n",
    "            question_embeddings = []\n",
    "#             context_embeddings = []\n",
    "            \n",
    "            for batch_sample in batch_samples:\n",
    "                \n",
    "                question = batch_sample[0]\n",
    "                context  = batch_sample[1]\n",
    "                answer   = batch_sample[2]\n",
    "                \n",
    "                total = question + context\n",
    "                cleaned_total   = total.split()\n",
    "                \n",
    "                total_embedding = getEmbeds(  session, cleaned_total, maxSeqLen_net1+maxSeqLen_net2)\n",
    "                question_embeddings.append(total_embedding)\n",
    "                \n",
    "                present_label = answer.split(\" \")\n",
    "                label_array = [0]*maxSeqLen_net2\n",
    "                index1 = int(present_label[0])\n",
    "                index2 = int(present_label[1])\n",
    "                index1 = min(index1,maxSeqLen_net2)\n",
    "                index2 = min(index2, maxSeqLen_net2)\n",
    "                \n",
    "                if(index1 == index2):\n",
    "                    label_array[index1-1] = 1\n",
    "                else:\n",
    "                    for i in range(index1, index2+1):\n",
    "                        label_array[i-1] = 1\n",
    "\n",
    "                labels.append(label_array)\n",
    "                \n",
    "            question_embeddings = np.asarray(question_embeddings)\n",
    "            labels = np.asarray(labels)\n",
    "            \n",
    "            yield question_embeddings, labels\n",
    "            \n",
    "            \n",
    "def generator(samples, session, batch_size = 32):\n",
    "    \n",
    "    num_samples = len(samples)\n",
    "    \n",
    "    while 1:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "#             batch_samples = samples[offset:offset+batch_size]\n",
    "#             labels = []\n",
    "                \n",
    "#             question_embeddings = np.zeros([batch_size, maxSeqLen_net1+maxSeqLen_net2, embed_size])\n",
    "#             labels = np.zeros([batch_size, maxSeqLen_net2])\n",
    "            q, c, l = get_batch(cnt=batch_size)\n",
    "#             print(q.shape, c.shape, l.shape)\n",
    "            q_c = np.concatenate([q, c], axis=1)\n",
    "            yield q_c, l\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ph_net1 = tf.placeholder(dtype = tf.float32, shape = (None, maxSeqLen_net1+maxSeqLen_net2 , embed_size))\n",
    "labels_placeholder = tf.placeholder(dtype = tf.int64, shape = (None, maxSeqLen_net2))\n",
    "\n",
    "with tf.variable_scope(\"rnn\"):    \n",
    "    weights = {\n",
    "        'w_inp'   : tf.get_variable(\"w_inp\", initializer = tf.contrib.layers.xavier_initializer(),   shape = [embed_size, hidden_size]),\n",
    "        'w_hidden': tf.get_variable(\"w_hidden\",initializer = tf.contrib.layers.xavier_initializer(),  shape = [hidden_size, hidden_size]),\n",
    "        'wfc1': tf.get_variable(\"wfc1\",initializer = tf.contrib.layers.xavier_initializer(), shape = [hidden_size, hidden_layer_size]),\n",
    "        'w_out':  tf.get_variable(\"w_out\",initializer = tf.contrib.layers.xavier_initializer(), shape = [hidden_layer_size, maxSeqLen_net2])\n",
    "    }\n",
    "    biases = {\n",
    "        'b_hidden': tf.get_variable(\"b_hidden\",initializer = tf.contrib.layers.xavier_initializer(), shape = [hidden_size]),\n",
    "        'bfc1': tf.get_variable(\"bfc1\",initializer = tf.contrib.layers.xavier_initializer(), shape = [hidden_layer_size]),\n",
    "        'b_out': tf.get_variable(\"b_out\",initializer = tf.contrib.layers.xavier_initializer(), shape = [maxSeqLen_net2])\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNNCell(tf.contrib.rnn.RNNCell):\n",
    "    \n",
    "    def __init__(self, input_size, state_size):\n",
    "        self.input_size  = input_size\n",
    "        self._state_size = state_size\n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "        \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._state_size\n",
    "        \n",
    "    def __call__(self, inputs, state):\n",
    "        \n",
    "        \n",
    "        state = tf.cast(state, tf.float32)\n",
    "        \n",
    "#         if(len(state.get_shape())<2):\n",
    "#             state = tf.expand_dims(state, axis=0)\n",
    "#         if(len(inputs.get_shape())<2):\n",
    "#             inputs = tf.expand_dims(inputs, axis=0)\n",
    "        \n",
    "        with tf.variable_scope(\"rnn\", reuse =True):\n",
    "                \n",
    "            W_x = tf.get_variable( 'w_inp', shape = (self.input_size, self.state_size), initializer = tf.contrib.layers.xavier_initializer(), dtype = tf.float32)\n",
    "            W_h = tf.get_variable( 'w_hidden' , shape = (self.state_size, self.state_size), initializer = tf.contrib.layers.xavier_initializer(), dtype = tf.float32)\n",
    "            b   = tf.get_variable( 'b_hidden' ,shape=(self.state_size), initializer = tf.contrib.layers.xavier_initializer(), dtype = tf.float32)\n",
    "            h_t = tf.tanh( tf.matmul(state, W_h) + tf.matmul(inputs, W_x) + b)\n",
    "            \n",
    "        new_state = h_t\n",
    "        \n",
    "        return new_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     with tf.variable_scope(\"RNN\", reuse = tf.AUTO_REUSE):\n",
    "#         rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "#         initial_state = rnn_cell.zero_state(batch_size, dtype = tf.float32)\n",
    "#         outputs,state = tf.nn.dynamic_rnn(rnn_cell, x, initial_state = initial_state, dtype = tf.float32)\n",
    "    \n",
    "def myNet1( x, state ):\n",
    "    \n",
    "    cell = RNNCell( embed_size, hidden_size)\n",
    "    x_ = tf.unstack( x, axis=1)\n",
    "    \n",
    "    for i in range(len(x_)):\n",
    "        state = cell( x_[i], state)\n",
    "        \n",
    "    return state\n",
    "    \n",
    "def FC_Net(input_fc):\n",
    "    with tf.variable_scope(\"rnn\"):\n",
    "        fc1 = tf.add( tf.matmul(input_fc, weights['wfc1']),biases['bfc1'] )\n",
    "        logits = tf.add( tf.matmul(fc1, weights['w_out']), biases['b_out'])\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1 shape is:  (64, 32)\n",
      "(64, 250)\n",
      "prediction 3:  (64, 250)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    state = tf.zeros(shape = [batch_size, hidden_size], dtype = tf.float32)\n",
    "    \n",
    "prediction_net1 = myNet1( input_ph_net1, state)\n",
    "print (\"Prediction 1 shape is: \",prediction_net1.get_shape())\n",
    "prediction3     = FC_Net(prediction_net1)\n",
    "print (prediction3.get_shape())\n",
    "\n",
    "pred_values = tf.argmax(prediction3,1)\n",
    "label_values = tf.argmax(labels_placeholder,1)\n",
    "\n",
    "correct_prediction = tf.equal( pred_values, label_values)\n",
    "accuracy_prediction = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print (\"prediction 3: \",prediction3.get_shape())\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( logits = prediction3, labels = labels_placeholder))\n",
    "\n",
    "## Optimizer. \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 1e-5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score(prediction, ground_truth):\n",
    "    \n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "        \n",
    "def evaluate(dataset, predictions):\n",
    "    \n",
    "    f1 = exact_match = total = 0\n",
    "    \n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                \n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    message = 'Unanswered question ' + qa['id'] + \\\n",
    "                              ' will receive score 0.'\n",
    "                    print(message, file=sys.stderr)\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                prediction = predictions[qa['id']]\n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Epoch:  0\n",
      "Itr: 0 , Loss: 9.71439  Accuracy is:  0.046875 Time:  0.62177491188\n",
      "Itr: 20 , Loss: 9.03849  Accuracy is:  0.015625 Time:  2.07964515686\n",
      "Itr: 40 , Loss: 9.32232  Accuracy is:  0.015625 Time:  2.05185604095\n",
      "Itr: 60 , Loss: 9.09136  Accuracy is:  0.0 Time:  2.05866479874\n",
      "Itr: 80 , Loss: 9.13525  Accuracy is:  0.03125 Time:  2.03135204315\n",
      "Itr: 100 , Loss: 9.10408  Accuracy is:  0.03125 Time:  2.0183570385\n",
      "Itr: 120 , Loss: 9.33399  Accuracy is:  0.0 Time:  2.01872396469\n",
      "Itr: 140 , Loss: 9.39105  Accuracy is:  0.015625 Time:  2.02227115631\n",
      "Itr: 160 , Loss: 9.32971  Accuracy is:  0.015625 Time:  2.02259683609\n",
      "Itr: 180 , Loss: 8.40777  Accuracy is:  0.0 Time:  2.03353118896\n",
      "Itr: 200 , Loss: 9.52085  Accuracy is:  0.0 Time:  2.02603292465\n",
      "Itr: 220 , Loss: 9.43702  Accuracy is:  0.03125 Time:  2.01594591141\n",
      "Itr: 240 , Loss: 9.02743  Accuracy is:  0.0 Time:  2.03871512413\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8fed4bc6e198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mbatchX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minput_ph_net1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatchX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatchY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-82252b45b2f0>\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m(samples, session, batch_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m#             question_embeddings = np.zeros([batch_size, maxSeqLen_net1+maxSeqLen_net2, embed_size])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m#             labels = np.zeros([batch_size, maxSeqLen_net2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;31m#             print(q.shape, c.shape, l.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mq_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-cfdcec7c8c13>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(cnt)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mq_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mq_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mc_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mc_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mlabel_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-cfdcec7c8c13>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(sent, fill, clean)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mPADDING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2VEC_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPADDING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_file = \"models/model1.ckpt\"\n",
    "\n",
    "with tf.Session(config = tf.ConfigProto(log_device_placement = True)) as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        print (\"number of Epoch: \", i)\n",
    "        batch_generator = generator(train_samples, sess, batch_size)\n",
    "        st = time.time()\n",
    "        for j in range( int(len(train_samples)/batch_size)):\n",
    "            batchX, batchY = batch_generator.next()\n",
    "            if(len(batchX)==batch_size):\n",
    "                sess.run(optimizer, feed_dict ={input_ph_net1: batchX, labels_placeholder: batchY})                \n",
    "                \n",
    "                if(j%20==0):\n",
    "                    loss_value, acc = sess.run( [loss, accuracy_prediction], feed_dict = {input_ph_net1: batchX, labels_placeholder: batchY})\n",
    "                    et = time.time()\n",
    "                    print ('Itr:', j,\", Loss:\",loss_value , \" Accuracy is: \",acc, \"Time: \", et-st)\n",
    "                    st = et\n",
    "                    \n",
    "            \n",
    "            \n",
    "#         valid_generator = generator( val_samples, sess, batch_size = len(val_samples))\n",
    "#         valX, valY = valid_generator.__next__()\n",
    "#         pred_logits = sess.run( prediction3, feed_dict = feed_dict={input_ph_net1: valX, labels_placeholder: valY})\n",
    "            \n",
    "#         print (\"The validation accuracy is: \",sess.run(loss, feed_dict={input_ph_net1: valX, labels_placeholder: valY}))\n",
    "#     saver.save( sess, \"model1.ckpt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
